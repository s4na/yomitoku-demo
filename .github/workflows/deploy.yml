name: Deploy to GitHub Pages

on:
  push:
    branches:
      - main
  workflow_dispatch:

permissions:
  contents: read
  pages: write
  id-token: write

concurrency:
  group: "pages"
  cancel-in-progress: false

jobs:
  deploy:
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: ğŸ Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: ğŸ’¾ Cache pip packages
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}-yomitoku-torch
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: ğŸ“¦ Install YomiToku
        run: |
          pip install yomitoku torch

      - name: ğŸ“¥ Get ONNX models from YomiToku cache
        run: |
          mkdir -p ./models
          python3 << 'EOF'
          import os
          import shutil
          from pathlib import Path

          print("ğŸ”„ Getting ONNX models from YomiToku...")

          # Check YomiToku version
          import yomitoku
          print(f"YomiToku version: {yomitoku.__version__ if hasattr(yomitoku, '__version__') else 'unknown'}")

          from yomitoku import DocumentAnalyzer

          # Initialize analyzer with infer_onnx=True to trigger ONNX model download
          print("\nğŸ“¦ Initializing DocumentAnalyzer with ONNX inference mode...")
          analyzer = DocumentAnalyzer(configs={"device": "cpu", "infer_onnx": True})
          print("âœ… DocumentAnalyzer initialized")

          # Find the cached ONNX models
          print("\nğŸ” Looking for cached ONNX models...")

          # Check HuggingFace cache directory
          home = Path.home()
          hf_cache = home / ".cache" / "huggingface" / "hub"

          detector_found = False
          recognizer_found = False

          if hf_cache.exists():
              print(f"ğŸ“‚ Searching in: {hf_cache}")
              # Search for ONNX files in HuggingFace cache
              for onnx_file in hf_cache.rglob("*.onnx"):
                  filename = onnx_file.name
                  if "detector" in str(onnx_file).lower() and not detector_found:
                      print(f"  Found detector: {onnx_file}")
                      shutil.copy2(onnx_file, "./models/text_detector.onnx")
                      detector_found = True
                  elif "recognizer" in str(onnx_file).lower() and not recognizer_found:
                      print(f"  Found recognizer: {onnx_file}")
                      shutil.copy2(onnx_file, "./models/text_recognizer.onnx")
                      recognizer_found = True

                  if detector_found and recognizer_found:
                      break

          # If not found in HF cache, try torch hub cache
          if not (detector_found and recognizer_found):
              torch_cache = home / ".cache" / "torch" / "hub"
              if torch_cache.exists():
                  print(f"ğŸ“‚ Searching in: {torch_cache}")
                  for onnx_file in torch_cache.rglob("*.onnx"):
                      filename = onnx_file.name
                      if "detector" in str(onnx_file).lower() and not detector_found:
                          print(f"  Found detector: {onnx_file}")
                          shutil.copy2(onnx_file, "./models/text_detector.onnx")
                          detector_found = True
                      elif "recognizer" in str(onnx_file).lower() and not recognizer_found:
                          print(f"  Found recognizer: {onnx_file}")
                          shutil.copy2(onnx_file, "./models/text_recognizer.onnx")
                          recognizer_found = True

                      if detector_found and recognizer_found:
                          break

          if not detector_found or not recognizer_found:
              print("\nâŒ Could not find ONNX models in cache")
              print("Available cache locations searched:")
              print(f"  - {hf_cache}")
              print(f"  - {torch_cache}")

              # List all ONNX files found
              print("\nAll .onnx files found:")
              for cache_dir in [hf_cache, torch_cache]:
                  if cache_dir.exists():
                      for f in cache_dir.rglob("*.onnx"):
                          print(f"  - {f}")

              raise FileNotFoundError("ONNX models not found in cache")

          # Verify file sizes
          detector_size = os.path.getsize("./models/text_detector.onnx")
          recognizer_size = os.path.getsize("./models/text_recognizer.onnx")

          print(f"\nâœ… Detector: {detector_size / 1024 / 1024:.2f} MB")
          print(f"âœ… Recognizer: {recognizer_size / 1024 / 1024:.2f} MB")
          print("âœ… All models retrieved successfully!")
          EOF

      - name: Setup Pages
        uses: actions/configure-pages@v4

      - name: Upload artifact
        uses: actions/upload-pages-artifact@v3
        with:
          path: '.'

      - name: Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v4
